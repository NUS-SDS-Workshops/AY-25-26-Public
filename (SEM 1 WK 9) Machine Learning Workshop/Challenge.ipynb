{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "* This is a fill in the blanks challenge\n",
        "* Basic Machine Learning functions have been written out for you\n",
        "* You will have to change the hyper-parameters yourself\n",
        "\n",
        "Feature Engineering\n",
        "* Some basic feature Engineering have been done for you\n",
        "* If you would like to do more, please feel free to do so\n",
        "* Some additional \"Options\" have been provided, simply copy & paste or uncomment to use the code"
      ],
      "metadata": {
        "id": "5pGmvSzko2cB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "cqakqVlfmHxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"mikhail1681/walmart-sales\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "Yrh2xuaY4-y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Engineering and Cleaning"
      ],
      "metadata": {
        "id": "p4EhhbjymStd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pro-Tip: Read Through the Lines in the code (& Explanation) before running the code block.\n",
        "\n",
        "#### If you want to back-track after doing feature engineering you need to re-import the dataset again."
      ],
      "metadata": {
        "id": "W_Je65Qen57g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the CSV into a Pandas Dataframe"
      ],
      "metadata": {
        "id": "3yNzTUdnnUfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(path + \"/Walmart_Sales.csv\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "3DUhVTQemV_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Feature Engineering"
      ],
      "metadata": {
        "id": "LpoK3_xTnZL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
        "df['Month'] = df['Date'].dt.month # Gives you a numeric month (1-12)\n",
        "df['Month_sin'] = np.sin(2 * np.pi * df[\"Month\"] / 12) # Wraps the month around. ie. January is Close to December\n",
        "df[\"Month_cos\"] = np.cos(2 * np.pi * df[\"Month\"] / 12)\n",
        "df.drop(columns=[\"Month\", \"Date\"], inplace=True) # Remove the month column from your dataset\n",
        "\n",
        "df.head()\n",
        "\n",
        "## Other Options (Uncomment or Copy & Paste to use the code):\n",
        "\n",
        "## df['Year'] = df['Date'].dt.year\n",
        "## df['Month'] = df['Date'].dt.month\n",
        "## df['Day'] = df['Date'].dt.day\n",
        "## df['DayOfWeek'] = df['Date'].dt.dayofweek"
      ],
      "metadata": {
        "id": "z9qDrQbLnTNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting into X, y and Train Test (80-20 split)\n",
        "\n",
        "\n",
        "\n",
        "*   DO NOT Change this train-test split. It will be used for evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "EhI8-jxVqH8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = df.sample(frac=0.8, random_state=1000)\n",
        "test = df.drop(train.index)\n",
        "\n",
        "X_train = train.drop(columns=[\"Weekly_Sales\"])\n",
        "y_train = train[\"Weekly_Sales\"]\n",
        "\n",
        "X_test = test.drop(columns=[\"Weekly_Sales\"])\n",
        "y_test = test[\"Weekly_Sales\"]"
      ],
      "metadata": {
        "id": "c6Rj-q08qC8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardisation of the Dataset"
      ],
      "metadata": {
        "id": "wc5-9s3wqDTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_standardised = scaler_X.fit_transform(X_train)\n",
        "X_train_standardised = pd.DataFrame(X_train_standardised, columns=X_train.columns) ## Restore the dataframe\n",
        "y_train_standardised = scaler_y.fit_transform(y_train.values.reshape(-1,1))\n",
        "\n",
        "X_test_standardised = scaler_X.transform(X_test)\n",
        "X_test_standardised = pd.DataFrame(X_test_standardised, columns=X_test.columns) ## Restore the dataframe\n",
        "y_test_standardised = scaler_y.transform(y_test.values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "tGP2HEcWpvfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning"
      ],
      "metadata": {
        "id": "o3j_m40CsEU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Fitting PCA"
      ],
      "metadata": {
        "id": "xOU00w95z5jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = X_train_standardised.shape[1], random_state= 1000)\n",
        "pca.fit(X_train_standardised)"
      ],
      "metadata": {
        "id": "kaKnWTC1zJxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Scree Plot"
      ],
      "metadata": {
        "id": "AUC9Z0CYz8Nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.arange(1,9) ,pca.explained_variance_ratio_, marker='o', linestyle='-', linewidth=2, markersize=6, color='steelblue')\n",
        "plt.xlabel('Principal Component Number', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Explained Variance Ratio', fontsize=12, fontweight='bold')\n",
        "plt.title('Scree Plot: Variance Explained by Each PC',\n",
        "              fontsize=14, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,9) ,np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='-', linewidth=2, markersize=6, color='steelblue')\n",
        "plt.xlabel('Principal Component Number', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Cumulative Explained Variance', fontsize=12, fontweight='bold')\n",
        "plt.title('Cumulative Plot: Sum of Each Individual PC',\n",
        "              fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MqTpTOWmz4xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Biplot"
      ],
      "metadata": {
        "id": "3z8RZm8h0d0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pcaX = pca.transform(X_train_standardised)[:, :2]   # first two principal components\n",
        "loadings = pca.components_.T[:, :2]    # (n_features, 2)\n",
        "print(\"Raw loadings values: \\n\", loadings)\n",
        "feature_names = X_train_standardised.columns\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 8)) # Create main figure and axis\n",
        "\n",
        "# Plot data points in a scatter plot using our first two PCs as the axis\n",
        "sc = ax1.scatter(\n",
        "    pcaX[:, 0], pcaX[:, 1],\n",
        "    c=y_train_standardised, cmap='coolwarm',\n",
        "    s=10, alpha=0.6, linewidth=0.3\n",
        ")\n",
        "\n",
        "# Label your primary axis -> PCA scores\n",
        "ax1.set_xlabel(\"PC1 scores\", fontsize=12, fontweight='bold', labelpad=10)\n",
        "ax1.set_ylabel(\"PC2 scores\", fontsize=12, fontweight='bold', labelpad=10)\n",
        "ax1.set_title(\"PCA Biplot with Score and Loading Axes\", fontsize=14, fontweight='bold', pad=25)\n",
        "\n",
        "# Create a secondary axis that records the loading of each feature in the original data set.\n",
        "ax2 = ax1.twinx().twiny()\n",
        "ax2.set_xlim(-1, 1)\n",
        "ax2.set_ylim(-1, 1)\n",
        "ax2.set_xlabel(\"PC1 loadings\", fontsize=12, fontweight='bold', color='red', labelpad=10)\n",
        "ax2.set_ylabel(\"PC2 loadings\", fontsize=12, fontweight='bold', color='red', labelpad=10)\n",
        "ax2.tick_params(axis='x', colors='red')\n",
        "ax2.tick_params(axis='y', colors='red')\n",
        "\n",
        "# Draw out the loading arrows.\n",
        "scale = (np.max(np.abs(pcaX)) / np.max(np.abs(loadings))) * 0.5\n",
        "for i, feature in enumerate(feature_names):\n",
        "    ax1.arrow(0, 0, loadings[i, 0]*scale, loadings[i, 1]*scale,\n",
        "              color='red', alpha=0.7, head_width=0.1, length_includes_head=True)\n",
        "    ax1.text(loadings[i, 0]*scale*1.1, loadings[i, 1]*scale*1.1, feature,\n",
        "             color='red', ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Add reference axis lines\n",
        "ax1.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "ax1.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "\n",
        "# Formatting and Layout Code\n",
        "xlim = ax1.get_xlim()\n",
        "ylim = ax1.get_ylim()\n",
        "x_pad = (xlim[1] - xlim[0]) * 0.15\n",
        "y_pad = (ylim[1] - ylim[0]) * 0.15\n",
        "ax1.set_xlim(-4,4)\n",
        "ax1.set_ylim(-5,5)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96]) # adjust the plot so that title is visable\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nY3ZwJBR0fVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UMAP"
      ],
      "metadata": {
        "id": "1HFUA4ng13Fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Fitting"
      ],
      "metadata": {
        "id": "oEqdGK2Q2ScC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "\n",
        "# from umap.umap_ import UMAP # uncomment if the above import does not work for you\n",
        "\n",
        "umap = UMAP(n_components=2, n_neighbors = 100, random_state=1000)\n",
        "umap_train = umap.fit_transform(X_train_standardised)"
      ],
      "metadata": {
        "id": "hZgmE3id14oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Visualisation"
      ],
      "metadata": {
        "id": "l46fiS4x2T-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: This code block uses KMeans Clustering because the y_variable is continuous. KMeans was not one of the unsupervised learning models in this Workshop, but remains extremely useful\n",
        "\n",
        "In its core essence, KMeans Clustering helps you identify clusters in your data by grouping data closest together.\n",
        "\n",
        "The main hyperparameter for KMeans is n_clusters, which is the number of clusters you would like to see\n",
        "\n",
        "Usually the number of clusters is eyeballed, but there are also other metrics that can guide you to find the best number of clusters, such as the elbow point (similar to PCA), Bayesian Information Criterion (BIC), Akaikean Information Criterion (AIC) or Akaikean Information Criterion Corrected (AICc).\n",
        "\n",
        "However, these metrics are simply a guide and you should mainly rely on your own intuition.\n",
        "\n",
        "For this dataset, it is quite clear that there are 3 main clusters"
      ],
      "metadata": {
        "id": "SsExvu6-34nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D / 100 Neighbours\n",
        "# Continuous Variable -> Using KMeans for better visualisation\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=1000) # feel free to toggle the number of clusters as you see fit\n",
        "labels = kmeans.fit(umap_train)\n",
        "\n",
        "plt.scatter(umap_train[:, 0], umap_train[:, 1], c=labels.labels_, s=5)\n",
        "plt.title(\"2D UMAP Visualization - 100 Neighbours\")\n",
        "plt.xlabel(\"Component 1\")\n",
        "plt.ylabel(\"Component 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WXPjhvJC2XE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) 3D Visualisation"
      ],
      "metadata": {
        "id": "YelomWrS6kt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "umap2 = UMAP(n_components=3, n_neighbors = 100, random_state=1000)\n",
        "umap_train2 = umap2.fit_transform(X_train_standardised)"
      ],
      "metadata": {
        "id": "RzngtN226kR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=1000) # feel free to toggle the number of clusters as you see fit\n",
        "labels = kmeans.fit(umap_train2)\n",
        "\n",
        "scatter = ax.scatter(\n",
        "    umap_train2[:, 0], umap_train2[:, 1], umap_train2[:, 2],\n",
        "    c=labels.labels_, s=5, alpha=0.7\n",
        ")\n",
        "\n",
        "ax.set_title(\"3D UMAP Visualization - 100 Neighbours\", fontsize=14)\n",
        "ax.set_xlabel(\"UMAP Dimension 1\")\n",
        "ax.set_ylabel(\"UMAP Dimension 2\")\n",
        "ax.set_zlabel(\"UMAP Dimension 3\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7NBQYdpn6uv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised Learning Comments:\n",
        "1. This dataset presents a case of a very informative unsupervised learning, where clusters are clearly defined. This is likely a case of non-linear data because UMAP produced a much better result than PCA\n",
        "2. The next step would be to think about comparing cluster characteristics to understand what is driving this divergence.\n",
        "3. There are a number of ways that you can do this, however, it is out of the scope of this workshop because there is simply not enough time and understanding the subsequent models will take up a whole workshop on its own.\n",
        "4. However, we strongly encourage you to read up on the subsequent steps to further understand your dataset! Listed below are some suggestions:\n",
        "* KMeans Group By Cluster\n",
        "* ANOVA / Kruskalâ€“Wallis tests --> Understand which features are driving the difference (These methods have strong foundations in Statistics)\n",
        "* Decision Trees with cluster as a target variable | This becomes a classification task | Important! Do not get the impression that models can only fall in the groups of supervised learning and unsupervised learning. In some cases (such as this one) supervised learning models can help with unsupervised learning tasks.\n",
        "* Principal Component Regression / Regression after UMAP"
      ],
      "metadata": {
        "id": "SUG94QTK7eXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning:"
      ],
      "metadata": {
        "id": "5vO-zX_GWlB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the provided code templates as a starting point. Experiment by modifying the model parameters and selected features to achieve the lowest possible RMSE."
      ],
      "metadata": {
        "id": "LpKZElNka52o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "atlXugTGYqsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- Choose regressors here(exclude Weekly_Sales) ---\n",
        "feature_cols = [\n",
        "\n",
        "]\n",
        "\n",
        "X_train_lr = X_train[feature_cols]\n",
        "X_test_lr = X_test[feature_cols]\n",
        "\n",
        "# --- Fit model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_lr, y_train)\n",
        "\n",
        "# --- Predict on test set ---\n",
        "y_pred_lr = model.predict(X_test_lr)\n",
        "\n",
        "# --- Calculate Mean Squared Error ---\n",
        "mse = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
      ],
      "metadata": {
        "id": "IG3o1mniWnFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "a_SKK8v2Y2tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- Choose regressors here(exclude Weekly_Sales) ---\n",
        "feature_cols = [\n",
        "\n",
        "]\n",
        "\n",
        "X_train_knn = X_train_standardised[feature_cols]\n",
        "X_test_knn = X_test_standardised[feature_cols]\n",
        "\n",
        "# --- Fit KNN Regressor ---\n",
        "knn = KNeighborsRegressor(n_neighbors = ____)  # Choose k value\n",
        "knn.fit(X_train_knn, y_train_standardised.ravel())\n",
        "\n",
        "# --- Predict on test set ---\n",
        "y_pred_standardised = knn.predict(X_test_knn)\n",
        "\n",
        "# --- Inverse transform predictions to original Weekly_Sales scale ---\n",
        "y_pred = scaler_y.inverse_transform(y_pred_standardised.reshape(-1, 1))\n",
        "y_test_original = scaler_y.inverse_transform(y_test_standardised)\n",
        "\n",
        "# --- Compute metrics ---\n",
        "mse = mean_squared_error(y_test_original, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"KNN Root Mean Squared Error (RMSE): {rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "tw_57xm9Y2b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "Kz3qAtOiZXLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# --- Choose regressors here(exclude Weekly_Sales) ---\n",
        "feature_cols = [\n",
        "\n",
        "]\n",
        "\n",
        "X_train_xgb = X_train[feature_cols]\n",
        "X_test_xgb = X_test[feature_cols]\n",
        "\n",
        "# --- Fit XGBoost Regressor (You may change the all the parameters except random_state within XGBRegressor() )---\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators = ___,     # number of boosting rounds (trees)\n",
        "    max_depth = ___,        # tree depth (controls model complexity)\n",
        "    learning_rate = ___,    # step size shrinkage\n",
        "    random_state = 1000     # Please don't change the random state for reproducibility\n",
        ")\n",
        "xgb_model.fit(X_train_xgb, y_train)\n",
        "\n",
        "# --- Predict on test set ---\n",
        "y_pred_xgb = xgb_model.predict(X_test_xgb)\n",
        "\n",
        "# --- Compute metrics ---\n",
        "mse = mean_squared_error(y_test, y_pred_xgb)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"XGBoost Root Mean Squared Error (RMSE): {rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "sQfo37ClYw6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "cZfVEY8-aSnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# --- Choose regressors here(exclude Weekly_Sales) ---\n",
        "feature_cols = [\n",
        "\n",
        "]\n",
        "\n",
        "X_train_rf = X_train[feature_cols]\n",
        "X_test_rf = X_test[feature_cols]\n",
        "\n",
        "# --- Fit Random Forest Regressor (You may change all the parameters except random_state within RandomForestRegressor()) ---\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators = ___,     # number of trees in the forest\n",
        "    max_depth = ___,        # let trees expand until all leaves are pure\n",
        "    random_state = 1000     # Please don't change the random state for reproducibility\n",
        ")\n",
        "rf_model.fit(X_train_rf, y_train)\n",
        "\n",
        "# --- Predict on test set ---\n",
        "y_pred_rf = rf_model.predict(X_test_rf)\n",
        "\n",
        "# --- Compute metrics ---\n",
        "mse = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"Random Forest Root Mean Squared Error (RMSE): {rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "DJ6HwaDOaQg0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
