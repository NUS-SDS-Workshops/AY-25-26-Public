{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Playground: Exploring Convolutional Neural Networks with MNIST\n",
        "\n",
        "This notebook is designed for hands-on experimentation with different CNN architectures. You can mix and match various layers to understand their impact on model learning and performance.\n",
        "\n",
        "## How to Use This Notebook:\n",
        "1. Run the setup cells (imports and data loading)\n",
        "2. Browse the **Layer Library** section\n",
        "3. Copy-paste layers into the `CustomCNN` class\n",
        "4. Train your model and observe the results\n",
        "5. Experiment with different combinations!"
      ],
      "metadata": {
        "id": "G8SH1U6UbDV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Imports"
      ],
      "metadata": {
        "id": "XvSh44SlbV-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "cdhJlh2AdbKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "4lLO4LhZdch2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "PAF3R_J-drDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def load_mnist_data(batch_size=64):\n",
        "    \"\"\"Load and prepare MNIST dataset\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f'Training samples: {len(train_dataset)}')\n",
        "    print(f'Test samples: {len(test_dataset)}')\n",
        "    print(f'Image shape: {train_dataset[0][0].shape}\\n')\n",
        "\n",
        "    return train_loader, test_loader, train_dataset, test_dataset\n",
        "\n",
        "\n",
        "def visualize_samples(dataset, num_samples=10):\n",
        "    \"\"\"Visualize sample images from dataset\"\"\"\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        img, label = dataset[i]\n",
        "        ax.imshow(img.squeeze(), cmap='gray')\n",
        "        ax.set_title(f'Label: {label}')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9PRnUiGQduxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Layer Library\n",
        "\n",
        "Copy and paste any layer into the `CustomCNN` class to experiment."
      ],
      "metadata": {
        "id": "WZ8s8ZFBd7h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "# CONVOLUTIONAL LAYER OPTIONS:\n",
        "#######################################\n",
        "\n",
        "# Basic Conv Layer (3x3 kernel, commonly used)\n",
        "# self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "\n",
        "# Medium Conv Layer (3x3 kernel)\n",
        "# self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "\n",
        "# Deep Conv Layer (3x3 kernel)\n",
        "# self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "# Large Kernel Conv (5x5 kernel, captures more spatial info)\n",
        "# self.conv_large = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2)\n",
        "\n",
        "# Small Kernel Conv (1x1 kernel, for channel dimension reduction)\n",
        "# self.conv_1x1 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1)\n"
      ],
      "metadata": {
        "id": "lwA9qc1od_C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "# POOLING LAYER OPTIONS:\n",
        "#######################################\n",
        "\n",
        "# Max Pooling (2x2, most common - keeps maximum values)\n",
        "# self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# Max Pooling (3x3, more aggressive downsampling)\n",
        "# self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "# Average Pooling (2x2, smoother than max pooling)\n",
        "# self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# Average Pooling (3x3)\n",
        "# self.avgpool3 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "# Global Average Pooling (reduces each feature map to a single value)\n",
        "# self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "# Global Max Pooling\n",
        "# self.global_maxpool = nn.AdaptiveMaxPool2d((1, 1))"
      ],
      "metadata": {
        "id": "Ihg4e-ZUebBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "# ACTIVATION FUNCTION OPTIONS:\n",
        "# Note: Can be used as layers or functions (F.relu(), etc.)\n",
        "#######################################\n",
        "\n",
        "# ReLU (most common, fast and effective)\n",
        "# self.relu = nn.ReLU()\n",
        "\n",
        "# Leaky ReLU (allows small negative values)\n",
        "# self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "# ELU (smoother than ReLU)\n",
        "# self.elu = nn.ELU(alpha=1.0)\n",
        "\n",
        "# GELU (used in transformers, smooth)\n",
        "# self.gelu = nn.GELU()\n",
        "\n",
        "# Sigmoid (outputs between 0 and 1)\n",
        "# self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "# Tanh (outputs between -1 and 1)\n",
        "# self.tanh = nn.Tanh()\n",
        "\n",
        "# Swish/SiLU (self-gated, smooth)\n",
        "# self.silu = nn.SiLU()"
      ],
      "metadata": {
        "id": "ENjJ-S5MejYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "# FULLY CONNECTED (LINEAR) LAYER OPTIONS:\n",
        "# Note: Input size depends on the output of previous layers!\n",
        "# For MNIST (28x28), after two 2x2 pooling layers: 7x7 spatial size\n",
        "# Example: 32 channels × 7 × 7 = 1568 input features\n",
        "#######################################\n",
        "\n",
        "# Large FC Layer (from flattened conv output to 512 units)\n",
        "# self.fc1 = nn.Linear(in_features=1568, out_features=512)\n",
        "\n",
        "# Medium FC Layer (512 to 256 units)\n",
        "# self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
        "\n",
        "# Small FC Layer (256 to 128 units)\n",
        "# self.fc3 = nn.Linear(in_features=256, out_features=128)\n",
        "\n",
        "# Tiny FC Layer (128 to 64 units)\n",
        "# self.fc4 = nn.Linear(in_features=128, out_features=64)\n",
        "\n",
        "# Output Layer (to 10 classes for MNIST)\n",
        "# self.fc_out = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "# Alternative smaller output path\n",
        "# self.fc_out_small = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "# Direct output (from conv features)\n",
        "# self.fc_direct = nn.Linear(in_features=1568, out_features=10)\n"
      ],
      "metadata": {
        "id": "X9aya-JDe1h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modelling Section"
      ],
      "metadata": {
        "id": "cPLOQX6-gu29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom CNN Model - Modify this class with your chosen layers!\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. Copy layers from the library above\n",
        "    2. Paste them in __init__\n",
        "    3. Define the forward pass using your layers\n",
        "    4. Remember to match input/output dimensions!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        # ====================================\n",
        "        # PASTE YOUR CHOSEN LAYERS HERE\n",
        "        # ====================================\n",
        "\n",
        "        # Example architecture (feel free to modify!):\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(in_features=32*7*7, out_features=128)\n",
        "        self.fc_out = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # ====================================\n",
        "        # DEFINE YOUR FORWARD PASS HERE\n",
        "        # ====================================\n",
        "\n",
        "        # Example forward pass (modify to match your layers!):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool(x)  # 28x28 -> 14x14\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool(x)  # 14x14 -> 7x7\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "VbO5QcVYgxta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_summary(model):\n",
        "    \"\"\"Print detailed model architecture and parameters\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"MODEL SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "\n",
        "    print(f\"{'Layer':<30} {'Type':<25} {'Params':<15}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for name, module in model.named_children():\n",
        "        num_params = sum(p.numel() for p in module.parameters())\n",
        "        num_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "        total_params += num_params\n",
        "        trainable_params += num_trainable\n",
        "        module_type = module.__class__.__name__\n",
        "        print(f\"{name:<30} {module_type:<25} {num_params:<15,}\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Calculate model size\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
        "    size_mb = (param_size + buffer_size) / 1024**2\n",
        "    print(f\"Model size: {size_mb:.2f} MB\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "_R_Zev5thFXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training and Evaluation"
      ],
      "metadata": {
        "id": "rm9VZlEzhKsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({'loss': running_loss/len(loader), 'acc': 100*correct/total})\n",
        "\n",
        "    return running_loss / len(loader), 100 * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    \"\"\"Evaluate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc='Evaluating'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), 100 * correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
        "    \"\"\"Complete training loop\"\"\"\n",
        "    train_losses, train_accs = [], []\n",
        "    test_losses, test_accs = [], []\n",
        "\n",
        "    print(\"Starting training...\\n\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return train_losses, train_accs, test_losses, test_accs\n"
      ],
      "metadata": {
        "id": "qmQepw6PhOWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualisation"
      ],
      "metadata": {
        "id": "gzmlPusxhQ6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(train_losses, train_accs, test_losses, test_accs):\n",
        "    \"\"\"Plot training and test metrics\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
        "    ax1.plot(test_losses, label='Test Loss', marker='s')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Test Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(train_accs, label='Train Accuracy', marker='o')\n",
        "    ax2.plot(test_accs, label='Test Accuracy', marker='s')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.set_title('Training and Test Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jtOzAMSghSpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Main Execution"
      ],
      "metadata": {
        "id": "F6yJS_HWhcsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"CNN PLAYGROUND FOR MNIST\")\n",
        "    print(\"=\"*70)\n",
        "    print()\n",
        "\n",
        "    # Load data\n",
        "    train_loader, test_loader, train_dataset, test_dataset = load_mnist_data(batch_size=64)\n",
        "\n",
        "    # Visualize samples\n",
        "    print(\"Visualizing sample data...\")\n",
        "    visualize_samples(train_dataset)\n",
        "\n",
        "    # Create model\n",
        "    print(\"\\nCreating model...\")\n",
        "    model = CustomCNN().to(device)\n",
        "    print_model_summary(model)\n",
        "\n",
        "    # Setup training\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    # Train model\n",
        "    num_epochs = 10\n",
        "    train_losses, train_accs, test_losses, test_accs = train_model(\n",
        "        model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device\n",
        "    )\n",
        "\n",
        "    # Visualize results\n",
        "    print(\"\\nPlotting training history...\")\n",
        "    plot_training_history(train_losses, train_accs, test_losses, test_accs)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXPERIMENT COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nEXPERIMENT IDEAS:\")\n",
        "    print(\"1. Try different layer combinations from the library\")\n",
        "    print(\"2. Experiment with depth (1-4 conv layers)\")\n",
        "    print(\"3. Compare different pooling strategies\")\n",
        "    print(\"4. Try different activation functions\")\n",
        "    print(\"\\nModify the CustomCNN class and run again!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ISkqTu6FheAq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}