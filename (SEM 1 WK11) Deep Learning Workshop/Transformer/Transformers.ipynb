{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhQEbPb0KHRQ"
      },
      "source": [
        "# Transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uroYCvt4B0Gi",
        "outputId": "0d85130a-4f90-4515-e2c1-042cee696a34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\haris\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Example\n",
        "import torch.nn as nn\n",
        "model = nn.Transformer(d_model=512,\n",
        "                       nhead=8,\n",
        "                       num_encoder_layers=6,\n",
        "                       num_decoder_layers=6)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Summariser\n",
        "\n",
        "**Note:** This is an **extractive summarizer** that uses transformer components to score and select \n",
        "existing sentences from the input text based on heuristics. It does NOT perform true learned \n",
        "summarization like BART or T5 (which can paraphrase, combine, or generate new sentences). \n",
        "This demonstrates transformer architecture without requiring training on large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRANSFORMER ARCHITECTURE DEMO\n",
            "\n",
            "This implements actual transformer components:\n",
            "- Multi-Head Self-Attention\n",
            "- Positional Encoding\n",
            "- Feed-Forward Networks\n",
            "- Layer Normalization\n",
            "- Residual Connections\n",
            "\n",
            "INPUT TEXT:\n",
            "Artificial intelligence has revolutionized many industries in recent years. \n",
            "    Machine learning algorithms can now process vast amounts of data to identify \n",
            "    patterns that humans might miss. The transformer architecture became a \n",
            "    breakthrough in natural language processing. It uses self-attention mechanisms \n",
            "    to weigh the importance of different words in a sequence. This allows the model \n",
            "    to capture long-range dependencies in text. Companies are now using transformers \n",
            "    for translation and summarization. The technology continues to evolve rapidly \n",
            "    with new models being released frequently.\n",
            "SUMMARY:\n",
            "Artificial intelligence has revolutionized many industries in recent years. The transformer architecture became a \n",
            "    breakthrough in natural language processing. Companies are now using transformers \n",
            "    for translation and summarization.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Transformer Model for Text Summarization\n",
        "\n",
        "\n",
        "This script implements a Transformer architecture using PyTorch, demonstrating\n",
        "the actual components used in models like BERT, GPT, and T5.\n",
        "\n",
        "IMPORTANT: This is an EXTRACTIVE summarizer - it selects and ranks existing sentences\n",
        "from the input text using transformer-based representations. It does NOT generate new \n",
        "text or paraphrase like modern seq2seq models (BART, T5, GPT). For true learned \n",
        "summarization, you would need a trained encoder-decoder model with a large dataset.\n",
        "\n",
        "What is a Transformer? \n",
        "\n",
        "A transformer is a neural network architecture that uses:\n",
        "1. Self-Attention: Learns which words relate to each other\n",
        "2. Multi-Head Attention: Multiple attention mechanisms working in parallel\n",
        "3. Positional Encoding: Encodes the position of words in the sequence\n",
        "4. Feed-Forward Networks: Deep learning layers that process information\n",
        "5. Layer Normalization: Keeps training stable\n",
        "6. Residual Connections: Helps gradient flow during training\n",
        "\n",
        "Unlike rule-based systems, transformers LEARN these patterns from data through training.\n",
        "\n",
        "Architecture Overview:\n",
        "INPUT TEXT → TOKENIZATION → EMBEDDINGS → ENCODER → DECODER → OUTPUT SUMMARY\n",
        "\n",
        "Components:\n",
        "- Encoder: Processes the input text, builds understanding\n",
        "- Decoder: Generates the summary based on encoder's understanding\n",
        "- Attention: Learns what to focus on at each step\n",
        "\n",
        "Note: This is a minimal transformer for educational purposes. Production models\n",
        "like GPT-4 or BERT have billions of parameters and are trained on massive datasets.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import math\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Encoding: Adds information about word positions.\n",
        "    \n",
        "    Why do we need this?\n",
        "    - Transformers process all words simultaneously (unlike RNNs which go one by one)\n",
        "    - Without position info, \"dog bites man\" = \"man bites dog\" to the model\n",
        "    - We add a unique position \"signature\" to each word's embedding\n",
        "    \n",
        "    How it works:\n",
        "    - Uses sine and cosine functions at different frequencies\n",
        "    - Creates a unique pattern for each position\n",
        "    - This pattern is added to the word embeddings\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Dimension of embeddings (e.g., 512)\n",
        "            max_len: Maximum sequence length we'll handle\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        # Create a matrix to hold positional encodings\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        \n",
        "        # Create the division term for the sine/cosine functions\n",
        "        # This creates different frequencies for different dimensions\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        # Apply sine to even indices, cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        # Add batch dimension\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        # Register as buffer (not a parameter, but part of model state)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add positional encoding to input embeddings.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Add positional encoding to input\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention: The core of the transformer!\n",
        "    \n",
        "    What is Attention?\n",
        "    - A mechanism to focus on relevant parts of the input\n",
        "    - Learns relationships between words (e.g., \"it\" refers to \"car\")\n",
        "    - Each word can attend to (look at) every other word\n",
        "    \n",
        "    Why Multi-Head?\n",
        "    - One attention head might focus on syntax (grammar)\n",
        "    - Another might focus on semantics (meaning)\n",
        "    - Another might track long-range dependencies\n",
        "    - Multiple heads capture different types of relationships\n",
        "    \n",
        "    How it works:\n",
        "    1. For each word, create Query (what I'm looking for), \n",
        "       Key (what I have), and Value (what information I carry)\n",
        "    2. Compare Queries with Keys to find relevance (attention scores)\n",
        "    3. Use attention scores to weight the Values\n",
        "    4. This tells each word what to pay attention to\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Dimension of embeddings (must be divisible by num_heads)\n",
        "            num_heads: Number of attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Dimension per head\n",
        "        \n",
        "        # Linear layers to create Queries, Keys, Values\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split embeddings into multiple heads.\n",
        "        \n",
        "        Changes shape from (batch, seq_len, d_model) \n",
        "        to (batch, num_heads, seq_len, d_k)\n",
        "        \"\"\"\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        return x.transpose(1, 2)\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Compute multi-head attention.\n",
        "        \n",
        "        Args:\n",
        "            query: What we're looking for\n",
        "            key: What we're comparing against\n",
        "            value: The information to extract\n",
        "            mask: Optional mask to prevent attention to certain positions\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "        \n",
        "        # 1. Linear projections to get Q, K, V\n",
        "        Q = self.W_q(query)\n",
        "        K = self.W_k(key)\n",
        "        V = self.W_v(value)\n",
        "        \n",
        "        # 2. Split into multiple heads\n",
        "        Q = self.split_heads(Q, batch_size)\n",
        "        K = self.split_heads(K, batch_size)\n",
        "        V = self.split_heads(V, batch_size)\n",
        "        \n",
        "        # 3. Compute attention scores\n",
        "        # Scores = Q * K^T / sqrt(d_k)\n",
        "        # This tells us how much each word should attend to every other word\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        \n",
        "        # 4. Apply mask if provided (e.g., for padding or future tokens)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # 5. Apply softmax to get attention weights (probabilities)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # 6. Apply attention to values\n",
        "        # This creates the weighted sum based on attention\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        # 7. Concatenate heads back together\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "        output = output.view(batch_size, -1, self.d_model)\n",
        "        \n",
        "        # 8. Final linear projection\n",
        "        output = self.W_o(output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed-Forward Network: Processes each position independently.\n",
        "    \n",
        "    What does it do?\n",
        "    - After attention tells us what to focus on, FFN processes that information\n",
        "    - Two linear layers with ReLU activation in between\n",
        "    - Applied to each position (word) separately\n",
        "    - Adds non-linearity and complexity to the model\n",
        "    \n",
        "    Think of it as:\n",
        "    - Attention = \"What should I look at?\"\n",
        "    - Feed-Forward = \"What should I do with what I'm looking at?\"\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Input/output dimension\n",
        "            d_ff: Hidden layer dimension (usually 4x d_model)\n",
        "            dropout: Dropout rate for regularization\n",
        "        \"\"\"\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply feed-forward network.\n",
        "        \n",
        "        x -> Linear -> ReLU -> Dropout -> Linear -> output\n",
        "        \"\"\"\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder Layer: One layer of the transformer encoder.\n",
        "    \n",
        "    Structure:\n",
        "    1. Multi-Head Self-Attention\n",
        "    2. Add & Normalize (residual connection + layer norm)\n",
        "    3. Feed-Forward Network\n",
        "    4. Add & Normalize\n",
        "    \n",
        "    Why residual connections?\n",
        "    - Helps gradients flow during training (prevents vanishing gradients)\n",
        "    - Allows model to learn identity function if needed\n",
        "    - Makes training deeper networks possible\n",
        "    \n",
        "    Why layer normalization?\n",
        "    - Stabilizes training\n",
        "    - Makes optimization easier\n",
        "    - Reduces internal covariate shift\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Process input through one encoder layer.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            mask: Optional attention mask\n",
        "        \"\"\"\n",
        "        # Self-attention with residual connection\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout1(attn_output)  # Residual connection\n",
        "        x = self.norm1(x)  # Layer normalization\n",
        "        \n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout2(ff_output)  # Residual connection\n",
        "        x = self.norm2(x)  # Layer normalization\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder: Stack of encoder layers.\n",
        "    \n",
        "    The encoder's job:\n",
        "    - Read and understand the input text\n",
        "    - Build rich representations of each word in context\n",
        "    - Output: contextualized embeddings that capture meaning\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: Size of vocabulary\n",
        "            d_model: Embedding dimension\n",
        "            num_heads: Number of attention heads\n",
        "            num_layers: Number of encoder layers to stack\n",
        "            d_ff: Feed-forward hidden dimension\n",
        "            max_len: Maximum sequence length\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        \n",
        "        # Convert token IDs to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        \n",
        "        # Stack of encoder layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Encode input sequence.\n",
        "        \n",
        "        Args:\n",
        "            x: Input token IDs (batch_size, seq_len)\n",
        "            mask: Optional attention mask\n",
        "        \"\"\"\n",
        "        # Convert tokens to embeddings\n",
        "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
        "        \n",
        "        # Add positional information\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Pass through all encoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleVocab:\n",
        "    \"\"\"\n",
        "    Simple vocabulary for tokenization.\n",
        "    \n",
        "    In real transformers:\n",
        "    - Use sophisticated tokenizers (BPE, WordPiece, SentencePiece)\n",
        "    - Handle subwords (e.g., \"unhappiness\" → \"un\", \"happiness\")\n",
        "    - Have vocabularies of 30k-50k tokens\n",
        "    \n",
        "    This is a simplified version for demonstration.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "        self.next_idx = 4\n",
        "    \n",
        "    def add_word(self, word):\n",
        "        \"\"\"Add a word to vocabulary.\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.next_idx\n",
        "            self.idx2word[self.next_idx] = word\n",
        "            self.next_idx += 1\n",
        "    \n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to token IDs.\"\"\"\n",
        "        words = text.lower().split()\n",
        "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in words]\n",
        "    \n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert token IDs back to text.\"\"\"\n",
        "        return ' '.join([self.idx2word.get(idx, '<UNK>') for idx in indices])\n",
        "\n",
        "\n",
        "class TransformerSummarizer:\n",
        "    \"\"\"\n",
        "    Main summarizer class that ties everything together.\n",
        "    \n",
        "    This implements extractive summarization using a real transformer encoder.\n",
        "    The encoder learns to understand the text, then we select important sentences.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model=128, num_heads=4, num_layers=2, d_ff=512):\n",
        "        \"\"\"\n",
        "        Initialize the transformer summarizer.\n",
        "        \n",
        "        Args:\n",
        "            d_model: Embedding dimension (smaller than production for demo)\n",
        "            num_heads: Number of attention heads\n",
        "            num_layers: Number of transformer layers\n",
        "            d_ff: Feed-forward hidden dimension\n",
        "        \"\"\"\n",
        "        self.vocab = SimpleVocab()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.encoder = None\n",
        "    \n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"Build vocabulary from training texts.\"\"\"\n",
        "        for text in texts:\n",
        "            words = text.lower().split()\n",
        "            for word in words:\n",
        "                self.vocab.add_word(word)\n",
        "    \n",
        "    def initialize_model(self):\n",
        "        \"\"\"Initialize the transformer encoder.\"\"\"\n",
        "        self.encoder = TransformerEncoder(\n",
        "            vocab_size=len(self.vocab.word2idx),\n",
        "            d_model=self.d_model,\n",
        "            num_heads=self.num_heads,\n",
        "            num_layers=self.num_layers,\n",
        "            d_ff=self.d_ff,\n",
        "            max_len=512,\n",
        "            dropout=0.1\n",
        "        )\n",
        "    \n",
        "    def extract_sentences(self, text):\n",
        "        \"\"\"Extract sentences from text.\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        return [s.strip() for s in sentences if s.strip()]\n",
        "    \n",
        "    def summarize(self, text, num_sentences=3):\n",
        "        \"\"\"\n",
        "        Summarize text using the transformer encoder.\n",
        "        \n",
        "        Note: Since we're not training this model, it won't produce good summaries.\n",
        "        This is to demonstrate the architecture. For real summarization, you'd need:\n",
        "        1. A large dataset of (document, summary) pairs\n",
        "        2. Training loop with loss function and optimizer\n",
        "        3. Many hours/days of training on GPUs\n",
        "        \n",
        "        Args:\n",
        "            text: Input text to summarize\n",
        "            num_sentences: Number of sentences to extract\n",
        "        \"\"\"\n",
        "        if self.encoder is None:\n",
        "            self.build_vocab([text])\n",
        "            self.initialize_model()\n",
        "        \n",
        "        sentences = self.extract_sentences(text)\n",
        "        \n",
        "        if len(sentences) <= num_sentences:\n",
        "            return text\n",
        "        \n",
        "        # Encode the full text\n",
        "        token_ids = self.vocab.encode(text)\n",
        "        input_tensor = torch.tensor([token_ids])\n",
        "        \n",
        "        # Get encoder outputs (contextualized embeddings)\n",
        "        with torch.no_grad():\n",
        "            encoder_output = self.encoder(input_tensor)\n",
        "        \n",
        "        # Score sentences based on their representation strength\n",
        "        # In a trained model, this would be learned. Here we use a heuristic.\n",
        "        sentence_scores = []\n",
        "        for idx, sentence in enumerate(sentences):\n",
        "            sent_tokens = self.vocab.encode(sentence)\n",
        "            if not sent_tokens:\n",
        "                continue\n",
        "            \n",
        "            # Get embeddings for this sentence's tokens\n",
        "            # Use mean pooling as a simple aggregation\n",
        "            sent_embeddings = encoder_output[0, :len(sent_tokens), :]\n",
        "            score = torch.norm(sent_embeddings.mean(dim=0)).item()\n",
        "            \n",
        "            sentence_scores.append((sentence, score, idx))\n",
        "        \n",
        "        # Select top sentences\n",
        "        top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:num_sentences]\n",
        "        top_sentences = sorted(top_sentences, key=lambda x: x[2])\n",
        "        \n",
        "        summary = '. '.join(sent[0] for sent in top_sentences) + '.'\n",
        "        return summary\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Demonstration of the real transformer architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    print(\"TRANSFORMER ARCHITECTURE DEMO\")\n",
        "    print(\"\\nThis implements actual transformer components:\")\n",
        "    print(\"- Multi-Head Self-Attention\")\n",
        "    print(\"- Positional Encoding\")\n",
        "    print(\"- Feed-Forward Networks\")\n",
        "    print(\"- Layer Normalization\")\n",
        "    print(\"- Residual Connections\")\n",
        "    \n",
        "    \n",
        "    # Sample text\n",
        "    sample_text = \"\"\"\n",
        "    Artificial intelligence has revolutionized many industries in recent years. \n",
        "    Machine learning algorithms can now process vast amounts of data to identify \n",
        "    patterns that humans might miss. The transformer architecture became a \n",
        "    breakthrough in natural language processing. It uses self-attention mechanisms \n",
        "    to weigh the importance of different words in a sequence. This allows the model \n",
        "    to capture long-range dependencies in text. Companies are now using transformers \n",
        "    for translation and summarization. The technology continues to evolve rapidly \n",
        "    with new models being released frequently.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"\\nINPUT TEXT:\")\n",
        "    print(sample_text.strip())\n",
        "    \n",
        "    # Create summarizer\n",
        "    summarizer = TransformerSummarizer(\n",
        "        d_model=128,      # Embedding dimension\n",
        "        num_heads=4,      # Number of attention heads\n",
        "        num_layers=2,     # Number of encoder layers\n",
        "        d_ff=512          # Feed-forward dimension\n",
        "    )\n",
        "    \n",
        "    # Generate summary\n",
        "    summary = summarizer.summarize(sample_text, num_sentences=3)\n",
        "    \n",
        "    print(\"SUMMARY:\")\n",
        "    print(summary)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_mode(loop=False):\n",
        "    \"\"\"\n",
        "    Interactive mode\n",
        "    \"\"\"\n",
        "    print(\"INTERACTIVE MODE - TRY YOUR OWN TEXT\")\n",
        "    while True:\n",
        "        if loop:\n",
        "            print(\"\\nOptions:\")\n",
        "            print(\"1. Enter your own text to summarize\")\n",
        "            print(\"2. Exit\")\n",
        "            \n",
        "            choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
        "            \n",
        "            if choice == \"2\":\n",
        "                print(\"\\nGoodbye!\")\n",
        "                break\n",
        "            elif choice != \"1\":\n",
        "                print(\"\\nInvalid choice. Please enter 1 or 2.\")\n",
        "                continue\n",
        "        \n",
        "        print(\"\\nEnter your text (press Enter twice when done):\")\n",
        "        \n",
        "        lines = []\n",
        "        empty_count = 0\n",
        "        while True:\n",
        "            line = input()\n",
        "            if line == \"\":\n",
        "                empty_count += 1\n",
        "                if empty_count >= 2:\n",
        "                    break\n",
        "            else:\n",
        "                empty_count = 0\n",
        "                lines.append(line)\n",
        "        \n",
        "        custom_text = \" \".join(lines).strip()\n",
        "        \n",
        "        if custom_text:\n",
        "            \n",
        "            # Create a new summarizer for the custom text\n",
        "            custom_summarizer = TransformerSummarizer(\n",
        "                d_model=128,\n",
        "                num_heads=4,\n",
        "                num_layers=2,\n",
        "                d_ff=512\n",
        "            )\n",
        "            \n",
        "            # Ask how many sentences they want\n",
        "            try:\n",
        "                num_sent = int(input(\"\\nHow many sentences in the summary? (default 3): \").strip() or \"3\")\n",
        "            except:\n",
        "                num_sent = 3\n",
        "            \n",
        "            custom_summary = custom_summarizer.summarize(custom_text, num_sentences=num_sent)\n",
        "            \n",
        "            print(\"YOUR SUMMARY:\")\n",
        "            print(custom_summary)\n",
        "            \n",
        "            # Statistics\n",
        "            original_sentences = len(custom_summarizer.extract_sentences(custom_text))\n",
        "            summary_sentences = len(custom_summarizer.extract_sentences(custom_summary))\n",
        "            \n",
        "            print(f\"Original: {original_sentences} sentences\")\n",
        "            print(f\"Summary: {summary_sentences} sentences\")\n",
        "            \n",
        "            # Exit after one summarization if loop=False\n",
        "            if not loop:\n",
        "                print(\"\\nDone!\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"\\nNo text entered. Please try again.\")\n",
        "            if not loop:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INTERACTIVE MODE - TRY YOUR OWN TEXT\n",
            "\n",
            "Enter your text (press Enter twice when done):\n",
            "YOUR SUMMARY:\n",
            "It's a vicious circle. The more useful our phones become, the more we use them. It's true for everyday tasks that are less high-stakes, too.\n",
            "Original: 19 sentences\n",
            "Summary: 3 sentences\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "interactive_mode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UB5CRlvLKxC"
      },
      "source": [
        "## Mini Transformer\n",
        "This exmaple trains a simple sentiment classifier on 10 movie reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ewPmtUiHLQsR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u5fXRrd5LTaR"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"review\": [\n",
        "        \"I loved this movie it was fantastic\",\n",
        "        \"Absolutely terrible film I hated it\",\n",
        "        \"This was a wonderful experience\",\n",
        "        \"The plot was boring and predictable\",\n",
        "        \"Amazing direction and great acting\",\n",
        "        \"Worst movie ever a total waste of time\",\n",
        "        \"Enjoyed every bit of this movie\",\n",
        "        \"The movie was awful and disappointing\",\n",
        "        \"One of the best films I have seen\",\n",
        "        \"Poorly written and badly acted\"\n",
        "    ],\n",
        "    \"label\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
        "}\n",
        "\n",
        "# We have text reviews paired with labels\n",
        "# Each review needs to be converted to numbers (computers can't understand words directly)\n",
        "# We'll create a \"vocabulary\" - a dictionary mapping words to numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "90gUPX5wLVXY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text) # Remove punctuation\n",
        "    return text.strip()\n",
        "\n",
        "df[\"review\"] = df[\"review\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPzQm5ZAO5lO"
      },
      "source": [
        "Why clean text?\n",
        "\n",
        "- Consistency: \"Good\", \"good\", and \"GOOD\" should be the same word\n",
        "- Simplicity: Remove unnecessary characters\n",
        "- Better learning: Model focuses on meaningful patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9dUiicGtLZh4"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLL3TuoALct9",
        "outputId": "a6452eff-589d-41c6-a529-296362caf32b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 36\n"
          ]
        }
      ],
      "source": [
        "words = set(\" \".join(train_df[\"review\"]).split())\n",
        "vocab = {\"<PAD>\": 0, \"<UNK>\": 1} #Special tokens\n",
        "for i, w in enumerate(sorted(words)):\n",
        "    vocab[w] = i + 2\n",
        "\n",
        "#Special tokens:\n",
        "# <PAD>: Padding token (fills empty spaces in shorter sentences)\n",
        "# <UNK>: Unknown token (for words not seen during training)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "max_len = 8  # short reviews --> fixed length 8\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "\n",
        "# Example Vocab:\n",
        "#     \"<PAD>\": 0,\n",
        "#     \"<UNK>\": 1,\n",
        "#     \"amazing\": 2,\n",
        "#     \"awful\": 3,\n",
        "#     \"movie\": 4,\n",
        "#     ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yHu7hHguLgEu"
      },
      "outputs": [],
      "source": [
        "def text_to_indices(text, vocab, max_len):\n",
        "  '''\n",
        "    Input: \"I loved this movie\"\n",
        "    Output: [45, 23, 67, 89, 0, 0, 0, 0]  # Padded to length 8\n",
        "  '''\n",
        "  tokens = [vocab.get(w, vocab[\"<UNK>\"]) for w in text.split()]\n",
        "  tokens = tokens[:max_len]\n",
        "  tokens += [vocab[\"<PAD>\"]] * (max_len - len(tokens))\n",
        "  return tokens\n",
        "\n",
        "X_train = torch.tensor([text_to_indices(t, vocab, max_len) for t in train_df[\"review\"]])\n",
        "y_train = torch.tensor(train_df[\"label\"].values, dtype=torch.float32)\n",
        "X_test = torch.tensor([text_to_indices(t, vocab, max_len) for t in test_df[\"review\"]])\n",
        "y_test = torch.tensor(test_df[\"label\"].values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_KG-Wt7JLijF"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "      super().__init__()\n",
        "      self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Add position information using sine and cosine functions\n",
        "      batch, seq_len, d_model = x.size()\n",
        "      pos = torch.arange(seq_len).unsqueeze(1)\n",
        "      div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
        "      pe = torch.zeros(seq_len, d_model)\n",
        "      pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "      pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "      return x + pe.unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qGIr2FzQEf4"
      },
      "source": [
        "Why do we need this?\n",
        "Attention mechanisms don't inherently understand word order. These two sentences would look identical without positional encoding:\n",
        "\n",
        "- \"The cat chased the dog\"\n",
        "- \"The dog chased the cat\"\n",
        "\n",
        "Positional encoding adds unique \"position fingerprints\" to each word using mathematical functions (sine/cosine waves at different frequencies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RawF-Qx_Ll9G"
      },
      "outputs": [],
      "source": [
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=16, num_heads=2):\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "      self.pos_encoding = PositionalEncoding(embed_dim)\n",
        "      self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "      self.ff = nn.Sequential(\n",
        "          nn.Linear(embed_dim, embed_dim * 2),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(embed_dim * 2, embed_dim)\n",
        "      )\n",
        "      self.fc = nn.Linear(embed_dim, 1)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "      mask = (x == 0)\n",
        "      x = self.embedding(x)\n",
        "      x = self.pos_encoding(x)\n",
        "      attn_out, _ = self.attn(x, x, x, key_padding_mask=mask)\n",
        "      x = x + attn_out  # residual connection\n",
        "      x = x + self.ff(x)\n",
        "      x = x.mean(dim=1)  # average pooling\n",
        "      return self.sigmoid(self.fc(x)).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xogyy7W6aaTU"
      },
      "source": [
        "Input Numbers -> Embeddings -> + Positional Encoding -> Self-Attention\n",
        "-> Feed-Forward -> Average Pooling -> Classification -> Output\n",
        "\n",
        "Each component's role:\n",
        "\n",
        "1. Embedding Layer:\n",
        "  - Converts word numbers into dense vectors\n",
        "\n",
        "  - Input: [4, 23, 67] (word IDs)\n",
        "Output: [[0.2, -0.5, ...], [0.1, 0.3, ...], ...] (vectors)\n",
        "\n",
        "\n",
        "2. Self-Attention: Lets words interact\n",
        "\n",
        "- \"not good\" → attention helps \"good\" consider \"not\"\n",
        "- Creates context-aware representations\n",
        "\n",
        "\n",
        "3. Feed-Forward Network: Processes attended information\n",
        "\n",
        "- Simple neural network:Linear -> ReLU -> Linear\n",
        "- Adds non-linear transformations\n",
        "\n",
        "\n",
        "4. Residual Connections: x = x + attention_output\n",
        "\n",
        "- Helps training by preserving original information\n",
        "- Prevents \"vanishing gradient\" problem\n",
        "\n",
        "\n",
        "5. Average Pooling: Combines all word vectors into one\n",
        "\n",
        "- Reduces sequence to single representation\n",
        "- Used for classification tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nCp_bhYLnkI",
        "outputId": "be4428db-1a7f-4cd7-a7e6-38e3bca7ce4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05 | Loss: 0.4433\n",
            "Epoch 10 | Loss: 0.0157\n",
            "Epoch 15 | Loss: 0.0000\n",
            "Epoch 20 | Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "model = MiniTransformer(vocab_size).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(X_train)\n",
        "    loss = criterion(preds, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1:02d} | Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbO4eHzsbMzS"
      },
      "source": [
        "**Training Process:**\n",
        "\n",
        "1. Forward Pass: Input flows through model → predictions\n",
        "2. Loss Calculation: Compare predictions to true labels\n",
        "3. Backward Pass: Calculate how to adjust weights (gradients)\n",
        "4. Weight Update: Optimizer adjusts model parameters\n",
        "5. Repeat: Do this for many epochs (full passes through data)\n",
        "\n",
        "**What's an Epoch?**\n",
        "\n",
        "- One complete pass through all training data\n",
        "- More epochs = more learning (but risk overfitting)\n",
        "\n",
        "**What's the Loss Function?**\n",
        "\n",
        "- BCELoss(Binary Cross-Entropy):\n",
        "- Measures prediction error\n",
        "- Lower loss = better predictions\n",
        "- Goal: Minimize this over training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HNAubJxLqnv",
        "outputId": "07750e96-c995-498a-e883-b9ba595abce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.50\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test)\n",
        "    preds_binary = (preds > 0.5).float()\n",
        "    accuracy = (preds_binary == y_test).float().mean().item()\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljv02nFAL6xL",
        "outputId": "a47ff315-341c-4dc6-d4ba-ae7deef1ca8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions:\n",
            "'I really enjoyed this movie' --> POSITIVE (1.00)\n",
            "'this was awful and boring' --> POSITIVE (1.00)\n",
            "'great film with wonderful acting' --> POSITIVE (1.00)\n",
            "'waste of time and energy' --> NEGATIVE (0.00)\n"
          ]
        }
      ],
      "source": [
        "examples = [\n",
        "    \"I really enjoyed this movie\",\n",
        "    \"this was awful and boring\",\n",
        "    \"great film with wonderful acting\",\n",
        "    \"waste of time and energy\"\n",
        "]\n",
        "\n",
        "print(\"Predictions:\")\n",
        "for sentence in examples:\n",
        "    tokens = torch.tensor([text_to_indices(clean_text(sentence), vocab, max_len)])\n",
        "    with torch.no_grad():\n",
        "        pred = model(tokens).item()\n",
        "    sentiment = \"POSITIVE\" if pred > 0.5 else \"NEGATIVE\"\n",
        "    print(f\"'{sentence}' --> {sentiment} ({pred:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1Cbt05ev7Q"
      },
      "source": [
        "# Transformer practice: Sentiment Classification\n",
        "\n",
        "Now lets scale up to a real dataset with 50,000 IMDB movie reviews.\n",
        "\n",
        "This transformer is slightly more complex and is a simplified version of an end to end transformer.\n",
        "\n",
        "**Disclaimer:**\n",
        "Please use **GPU** to run the code blocks below. Using CPU will result in very long training time. If you do not have GPU locally in your device you can use use google Colab gpu by changing runtime type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KlNi4jRAsjLM"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01moptim\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mre\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyVgnBCLwzhd"
      },
      "outputs": [],
      "source": [
        "# Load the IMDB dataset\n",
        "# Note: Update the path if your CSV file is in a different location\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWC-KpCKxHkw",
        "outputId": "a29f9208-5bbf-4358-9951-7a576d66cd19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 50000\n",
            "Columns: ['review', 'sentiment']\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dX31ZAnoybmO",
        "outputId": "1210037c-e7da-453e-8ccb-4524e7174069"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-67927748-5767-459b-8925-c69c6c10361b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67927748-5767-459b-8925-c69c6c10361b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-67927748-5767-459b-8925-c69c6c10361b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-67927748-5767-459b-8925-c69c6c10361b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-554ec717-10ee-4086-8baf-a94792d49775\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-554ec717-10ee-4086-8baf-a94792d49775')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-554ec717-10ee-4086-8baf-a94792d49775 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ_3JWj6ygid"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    text = str(text).lower()\n",
        "    # Replace <br /> tags with spaces\n",
        "    text = re.sub(r'<br\\s*/?\\s*>', ' ', text)\n",
        "    # Remove other HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHEWHE6wcR8p"
      },
      "source": [
        "Real-world data is messy\n",
        "\n",
        "This handles:\n",
        "- HTML tags\n",
        "- Special characters:@, #, $, %, &\n",
        "- Multiple spaces and formatting issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ0FtGbHemZw"
      },
      "outputs": [],
      "source": [
        "df['review'] = df['review'].apply(preprocess_text)\n",
        "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_oSZkCNzzt_R",
        "outputId": "3ed5b4f8-00fb-4815-f6a7-664f1f993772"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49577,\n        \"samples\": [\n          \"i saw bits and pieces of this on tv once and when a friend recommended it i began looking for it even though it seemed no place nearby had it i finally got a hold of it in an antique store and couldnt wait to watch itoh that i had seen it a couple years earlier and couldve really enjoyed it i was surprised that this movie was only or so minutes long and i think this is what made the plot and story so lacking the plot really does sound like a good one both on the trailer and the movie comments a teenager angus jesse bradford and his newfound stray lab yellow are marooned on an island during a storm on a boat trip with his father bruce davison together they manage to survive the wilderness and wait to be found and rescued still what is never mentioned is that everything is shortened and the events of the plot are very rushed there is a possible love interest between angus and sara but theyre never shown together for more than a moment yellow is a mischievous dog the parents are reluctant to keep but in a few days he seems to be appreciated enough to join a boat trip the scene of the mother mimi rogers mentioning vaguely what death is like to the younger boy joel palmer doesnt go anywhere in no time we learn that days have been spent on the island then suddenly its then of all the animals a castaway could be exposed to in the wild only kind a wolf attacks them why couldnt something else have been a problem instead of having the same type of animal maybe even the same one strike twice there are few views of how angus prepares food except when he discovers fruit and roots and when he roasts a trapped rat if he knows so much about survival skills why werent more scenes with it shown the one thing that made me blank was why the dog didnt have much part alone when he is rescued and the dog is left behind on the island there is no scene showing how he survives without a humans help i wished i was more open to this when watching it but i did enjoy some of this the acting was good and the score was enjoyable though i found myself wondering why the father looked so much older than his family and why he and the main search and rescue conductor share names this is a good movie for kids but though the protagonist is nobody over would be interested with this\",\n          \"i must confess to not having read the original m r james story although i have read many of his other supernatural tales ive also seen most of the previous bbc christmas ghost stories and this one in my opinion surpasses most of them only equalling the signalman i cant really fault a view from a hill the direction and mood is perfect as is the acting lighting and of course the story and writing i thoroughly enjoyed this and can only hope for more of this quality from the same director and production team i understand that the bbc plan to make some more not necessarily based on m r james stories so thats promising\",\n          \"i mean seriously what group would sing about a crazy car so what if their ten its way too immature for a little kid to sing about being my women i mean seriously the name is pretty corny too naked brothers just because they take off their pants how creativei dont get why they need a tv show i mean most artist dont really need a tv show about themselves especially the naked brothers band heck how many of them are in the freaking group and seriously whats with the movie jeez nick use to be the hightlight of my years growing up but seriously the naked brother band so many parents would not let their kids watch this especially with the name the naked brothers band its a stupid uncreative show that should not be aired onto tv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-effbe7c4-8f14-4012-ad61-df4af5dc0fdf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production the filming tech...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically theres a family where a little boy j...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter matteis love in the time of money is a ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-effbe7c4-8f14-4012-ad61-df4af5dc0fdf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-effbe7c4-8f14-4012-ad61-df4af5dc0fdf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-effbe7c4-8f14-4012-ad61-df4af5dc0fdf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f1a571ce-ecc1-4a97-8fc7-69ef126ed5e2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1a571ce-ecc1-4a97-8fc7-69ef126ed5e2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f1a571ce-ecc1-4a97-8fc7-69ef126ed5e2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              review sentiment  label\n",
              "0  one of the other reviewers has mentioned that ...  positive      1\n",
              "1  a wonderful little production the filming tech...  positive      1\n",
              "2  i thought this was a wonderful way to spend ti...  positive      1\n",
              "3  basically theres a family where a little boy j...  negative      0\n",
              "4  petter matteis love in the time of money is a ...  positive      1"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAVc-iOM0bHi"
      },
      "outputs": [],
      "source": [
        "# First split: separate test set (80% for training+validation, 20% for final test)\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_RpaUrz4kIV"
      },
      "outputs": [],
      "source": [
        "# Second split: separate validation set (80% train, 20% validation)\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df, test_size=0.2, random_state=42, stratify=train_val_df['label']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkU8r5CMcKKN"
      },
      "source": [
        "Why three splits?\n",
        "\n",
        "- Training set: Model learns from this\n",
        "- Validation set: Check performance during training (tune hyperparameters)\n",
        "- Test set: Final evaluation (never seen during training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhwNWr-l44fu",
        "outputId": "70309267-4740-40ec-80f8-4e8a8d97aae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Training: 32000\n",
            "  Validation: 8000\n",
            "  Test: 10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"  Training: {len(train_df)}\")\n",
        "print(f\"  Validation: {len(val_df)}\")\n",
        "print(f\"  Test: {len(test_df)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ak6eQ7o9jqk"
      },
      "outputs": [],
      "source": [
        "# A vocabulary is a dictionary that maps each word to a unique number\n",
        "# We only use the TRAINING data to build it (to prevent data leakage)\n",
        "\n",
        "words = set()  # Use a set to collect unique words\n",
        "for review in train_df['review']:\n",
        "    words.update(review.split())  # Split review into words\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "vocab = {\n",
        "    '<PAD>': 0,  # Special token for padding short sentences\n",
        "    '<UNK>': 1   # Special token for unknown words\n",
        "}\n",
        "\n",
        "# Add all words from training data\n",
        "for idx, word in enumerate(sorted(list(words))):\n",
        "    vocab[word] = idx + 2  # Start from 2 (0 and 1 are special tokens)\n",
        "\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kwgKTGnfv1c",
        "outputId": "9a2a1469-9dc3-4951-dcd9-29d552449eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: torch.Size([32000, 580])\n",
            "X_val shape: torch.Size([8000, 580])\n",
            "X_test shape: torch.Size([10000, 580])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Neural networks can't understand words directly, we need to convert them to numbers!\n",
        "\n",
        "review_lengths = train_df['review'].apply(lambda x: len(x.split()))\n",
        "max_length = int(review_lengths.quantile(0.95))\n",
        "\n",
        "# Why not use the longest review?\n",
        "\n",
        "# Outliers waste computation\n",
        "# 95% coverage balances efficiency and completeness\n",
        "# Typical result: ~400 words instead of 2000+\n",
        "\n",
        "def text_to_numbers(sentence, vocab, max_len=max_length):\n",
        "    \"\"\"\n",
        "    Convert a sentence to a list of numbers.\n",
        "    Each word becomes its number from the vocabulary.\n",
        "    Unknown words become <UNK> token.\n",
        "    Sequences are padded/truncated to max_len\n",
        "    \"\"\"\n",
        "    # Convert each word to its vocabulary number\n",
        "    tokens = [vocab.get(word, vocab['<UNK>']) for word in sentence.split()]\n",
        "\n",
        "    # Pad if too short (add zeros at the end)\n",
        "    if len(tokens) < max_len:\n",
        "        tokens = tokens + [vocab['<PAD>']] * (max_len - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:max_len]\n",
        "    return tokens\n",
        "\n",
        "# Convert all reviews to numbers\n",
        "X_train = torch.tensor([text_to_numbers(r, vocab, max_length) for r in train_df['review']])\n",
        "X_val = torch.tensor([text_to_numbers(r, vocab, max_length) for r in val_df['review']])\n",
        "X_test = torch.tensor([text_to_numbers(r, vocab, max_length) for r in test_df['review']])\n",
        "\n",
        "y_train = torch.tensor(train_df['label'].values, dtype=torch.float32)\n",
        "y_val = torch.tensor(val_df['label'].values, dtype=torch.float32)\n",
        "y_test = torch.tensor(test_df['label'].values, dtype=torch.float32)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHWTU0GK6mX6"
      },
      "outputs": [],
      "source": [
        "# DataLoaders help us process data in small batches instead of all at once\n",
        "# This is more efficient and allows training on large datasets\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Simple dataset wrapper for PyTorch\n",
        "    \"\"\"\n",
        "    def __init__(self, reviews, labels):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.reviews[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = IMDBDataset(X_train, y_train)\n",
        "val_dataset = IMDBDataset(X_val, y_val)\n",
        "test_dataset = IMDBDataset(X_test, y_test)\n",
        "\n",
        "# Create loaders (batch_size = how many reviews to process at once)\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqivWDdgc0Fd"
      },
      "source": [
        "What's a DataLoader?\n",
        "\n",
        "- Feeds data in small batches (e.g., 64 reviews at a time)\n",
        "- Shuffles data each epoch (prevents learning order patterns)\n",
        "- Handles memory efficiently (can't load 50K reviews at once!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVy83ukT-pDj"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Encoding adds information about word position.\n",
        "\n",
        "    Why? Self-attention doesn't care about word order by default.\n",
        "    \"I hate this movie\" vs \"This movie I hate\" would look the same!\n",
        "\n",
        "    Positional encoding fixes this by adding position information to each word.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, max_len=max_length):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Create a matrix of shape (max_len, embed_dim)\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "\n",
        "        # Use sine and cosine functions at different frequencies\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() *\n",
        "                             (-np.log(10000.0) / embed_dim))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Even dimensions\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd dimensions\n",
        "\n",
        "        # Register as buffer (part of model but not a learnable parameter)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to embeddings\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOI-Sxv2c6qe"
      },
      "source": [
        "Mathematical Intuition:\n",
        "\n",
        "- Uses sine and cosine waves at different frequencies\n",
        "- Each position gets a unique \"signature\"\n",
        "- Similar positions have similar encodings\n",
        "- Model can learn relative positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnZUefVi-xlM"
      },
      "outputs": [],
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A Simple Transformer for Sentiment Analysis\n",
        "\n",
        "    Architecture:\n",
        "    1. Embedding Layer: Converts word numbers to dense vectors\n",
        "    2. Positional Encoding: Adds position information\n",
        "    3. Self-Attention: Lets words \"look at\" other words in the sentence\n",
        "    4. Feed-Forward Network: Processes the attended information\n",
        "    5. Classification Head: Makes the final positive/negative decision\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim=64, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # COMPONENT 1: WORD EMBEDDINGS\n",
        "        # Converts each word number into a vector of size embed_dim\n",
        "        # Example: word #234 -> [0.2, -0.5, 0.1, ...] (64 numbers)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # COMPONENT 2: POSITIONAL ENCODING\n",
        "        # Adds position information so the model knows word order\n",
        "        self.pos_encoding = PositionalEncoding(embed_dim, max_len=max_length)\n",
        "\n",
        "        # COMPONENT 3: SELF-ATTENTION\n",
        "        # This is the KEY component of transformers\n",
        "        # It lets each word \"attend to\" (look at) other words\n",
        "        # Example: In \"The movie was not good\", \"not\" attends to \"good\"\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,      # Size of embeddings\n",
        "            num_heads,      # Number of attention heads (parallel attention mechanisms)\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # COMPONENT 4: FEED-FORWARD NETWORK (FFN)\n",
        "        # Processes the information from attention\n",
        "        # Just a simple neural network: Linear -> ReLU -> Dropout -> Linear\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),  # Expand\n",
        "            nn.ReLU(),                             # Activation\n",
        "            nn.Dropout(dropout),                   # Regularization\n",
        "            nn.Linear(embed_dim * 4, embed_dim)   # Compress back\n",
        "        )\n",
        "\n",
        "        # COMPONENT 5: LAYER NORMALIZATION\n",
        "        # Helps with training stability (normalizes the values)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # COMPONENT 6: DROPOUT\n",
        "        # Randomly drops some connections during training to prevent overfitting\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # COMPONENT 7: CLASSIFIER\n",
        "        # Final layers that make the positive/negative decision\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, 1)  # Output: single number for binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: how data flows through the model\n",
        "        Input: x = batch of reviews as numbers, shape (batch_size, max_length)\n",
        "        Output: predictions (batch_size,) where each value is between 0 and 1\n",
        "        \"\"\"\n",
        "\n",
        "        # Create padding mask\n",
        "        # We need to ignore padded tokens (the zeros we added)\n",
        "        padding_mask = (x == 0)  # True where there's padding\n",
        "\n",
        "        # Convert word numbers to embeddings\n",
        "        # Shape: (batch_size, max_length, embed_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Add positional information\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # Self-Attention Block (with residual connection)\n",
        "        # The attention mechanism lets words interact with each other\n",
        "        attn_out, _ = self.attention(x, x, x, key_padding_mask=padding_mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_out))  # Add & Norm (residual connection)\n",
        "\n",
        "        # Feed-Forward Block (with residual connection)\n",
        "        # Process the attended information\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_out))  # Add & Norm (residual connection)\n",
        "\n",
        "        # Average Pooling\n",
        "        # Combine all word representations into one vector\n",
        "        # We ignore padded positions by masking them out\n",
        "        mask_expanded = (~padding_mask).unsqueeze(-1).float()\n",
        "        x = (x * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1).clamp(min=1)\n",
        "\n",
        "        # Classification\n",
        "        # Pass through classifier to get final prediction\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        # Sigmoid activation\n",
        "        # Converts output to probability between 0 and 1\n",
        "        return torch.sigmoid(x).squeeze()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hYtg29WdaAf"
      },
      "source": [
        "Multi-Head Attention (num_heads=4)\n",
        "Think of it as having 4 different reading strategies simultaneously:\n",
        "\n",
        "- Head 1: Focuses on nearby words (local context)\n",
        "- Head 2: Looks for negations (\"not\", \"never\")\n",
        "- Head 3: Identifies subject-verb relationships\n",
        "- Head 4: Captures overall sentiment patterns\n",
        "\n",
        "\n",
        "Input: \"The movie was not very good\"\n",
        "\n",
        "Head 1 attention: \"not\" <--> \"good\" (negation)\n",
        "\n",
        "Head 2 attention: \"movie\" <--> \"good\" (subject-attribute)\n",
        "\n",
        "Head 3 attention: \"was\" <--> \"good\" (temporal context)\n",
        "\n",
        "Head 4 attention: Overall sentence sentiment\n",
        "\n",
        "**Layer Normalisation**\n",
        "\n",
        "Purpose: Stabilizes training\n",
        "\n",
        "- Normalizes values to have mean=0, std=1\n",
        "- Prevents values from getting too large or small\n",
        "- Makes training faster and more stable\n",
        "\n",
        "**Dropout Regularisation**\n",
        "\n",
        "Purpose: Prevents overfitting\n",
        "\n",
        "- Randomly \"turns off\" 10% of neurons during training\n",
        "- Forces model to not rely on specific neurons\n",
        "- Creates more robust, generalizable model\n",
        "\n",
        "\n",
        "**FFN**\n",
        "Architecture Pattern:\n",
        "\n",
        "1. Expand: Increase dimensionality (more expressive)\n",
        "2. Activate: ReLU adds non-linearity (learns complex patterns)\n",
        "3. Compress: Return to original size\n",
        "\n",
        "Why expand then compress?\n",
        "\n",
        "- Creates a \"bottleneck\" that forces learning of important features\n",
        "- More parameters = more learning capacity\n",
        "- Used in every Transformer layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33U7ImIjFH4f"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleTransformer(vocab_size).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWgZJHK2FgKM",
        "outputId": "d696e828-6b81-4d4b-a28d-0bbfa7413f32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:27<00:00, 18.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3] - Loss: 0.5108\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:26<00:00, 18.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/3] - Loss: 0.3123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:26<00:00, 18.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/3] - Loss: 0.2338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for reviews, labels in tqdm(train_loader):\n",
        "        reviews, labels = reviews.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(reviews)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {total_loss / len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzdlB9IpFkMo"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for reviews, labels in test_loader:\n",
        "        reviews, labels = reviews.to(device), labels.to(device)\n",
        "        preds = model(reviews)\n",
        "        preds = (preds > 0.5).float()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "# Print test accuracy\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")\n",
        "print(f\"Correct predictions: {correct}/{total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KlfsdwBtb5A",
        "outputId": "4f8b93a1-63dd-419b-96cc-501d1b4ad44b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'amazing wonderful best' → POSITIVE (1.00)\n",
            "'terrible awful worst' → NEGATIVE (0.00)\n",
            "'love great film' → POSITIVE (1.00)\n",
            "'this movie made it into one of my top 10 most awful movies' → NEGATIVE (0.00)\n",
            "'fantastic acting and story' → POSITIVE (1.00)\n",
            "'poorly written and boring' → NEGATIVE (0.00)\n"
          ]
        }
      ],
      "source": [
        "test_sentences = [\n",
        "    \"amazing wonderful best\",\n",
        "    \"terrible awful worst\",\n",
        "    \"love great film\",\n",
        "    \"this movie made it into one of my top 10 most awful movies\",\n",
        "    \"fantastic acting and story\",\n",
        "    \"poorly written and boring\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    tokens = torch.tensor([text_to_numbers(sentence, vocab)])\n",
        "    with torch.no_grad():\n",
        "        pred = model(tokens.to(device)).item()\n",
        "    sentiment = \"POSITIVE\" if pred > 0.5 else \"NEGATIVE\"\n",
        "    print(f\"'{sentence}' -> {sentiment} ({pred:.2f})\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (My Environment)",
      "language": "python",
      "name": "my_env_name"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
