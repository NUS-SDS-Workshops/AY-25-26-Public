{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **What's Ur EDA? | Exploratory Data Analysis I**\n",
        "\n",
        "Hello! Welcome to your first ipynb file for this workshop! The language we are using is Python. Here, we will cover data inspection and data cleaning techniques.\n",
        "After this segment, you will learn how to:\n",
        "1. Read a csv file into the Python environment\n",
        "2. Investigate the features of your dataset\n",
        "3. Deal with duplicated values, inconsistent labelling, missing and erroneous values\n",
        "4. Generate a Profile Report with YData as an alternative\n",
        "\n",
        "Please ensure that you have downloaded \"employee_dirty.csv\" and uploaded them (right click) on the side under \"Files\". You can use the \"Table of Contents\" tab at the side to help with navigation.\n",
        "\n",
        "The original csv file can be found from https://www.kaggle.com/datasets/tawfikelmetwally/employee-dataset.\n",
        "\n",
        "Let's begin!"
      ],
      "metadata": {
        "id": "I3t9dyVEzuZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import relevant libraries"
      ],
      "metadata": {
        "id": "7C1dSZLCTTbb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7fok0NB1H8J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # used for data manipulation & analysis\n",
        "import numpy as np   # super fast calculator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dataset"
      ],
      "metadata": {
        "id": "sEpk-zVzTOqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"employee_dirty.csv\") # Check if the first row is a header. If not, header=None"
      ],
      "metadata": {
        "id": "2bUjBObhTP8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Inspection\n",
        "\n",
        "Before we begin on data cleaning, we will run some codes to understand the features of our dataset. df.head() provides a quick overview of your data's structure and content.\n",
        "\n",
        "To customise the number of rows you wish to see, you can add a numerical argument into the parentheses.\n",
        "For instance, df.head(10) gives the first 10 rows of your dataframe."
      ],
      "metadata": {
        "id": "gYml9VwO8zpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows the first 5 rows of dataframe & column headers\n",
        "df.head()"
      ],
      "metadata": {
        "id": "gQYGI1AJ9G9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you may consider looking at the last few rows."
      ],
      "metadata": {
        "id": "YqeMxKMSAnxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows the last 5 rows\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "b-kBz0WqArup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shape"
      ],
      "metadata": {
        "id": "EmF5CySll0BJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may want to look at the number of rows and columns in the dataframe.\n",
        "\n",
        "The result (x, y) indicates the following: (no. of rows, no. of columns)"
      ],
      "metadata": {
        "id": "1d5kbOILBU4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No. of rows & columns\n",
        "df.shape  # attributes have no ()"
      ],
      "metadata": {
        "id": "6u1Twr86BHmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Types"
      ],
      "metadata": {
        "id": "qzMtAPHsl1ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may also wish to look at the data types of our columns to identify if they are numeric, strings or dates.\n",
        "- Numeric: int64, float64\n",
        "- String: object (typically strings)\n",
        "- Date/Time: datetime64\n",
        "\n"
      ],
      "metadata": {
        "id": "SblaIAszBrhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data types of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "OKQgEkdgB2M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Summary"
      ],
      "metadata": {
        "id": "ORgNvfMXl60q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain a more comprehensive (and useful) summary of the dataframe, we can use df.info() which gives us\n",
        "1. No. of entries / rows\n",
        "2. Column Information (column names, non-null count, data type)\n",
        "3. Memory Usage of the Dataframe\n",
        "\n",
        "From the non-null count, we can infer if there are missing values. Non-Null Count column tells you how many rows in that column **do not** contain missing values (NaN -- Not a Number or None)"
      ],
      "metadata": {
        "id": "KxQvoU-a_pdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concise summary of dataframe\n",
        "df.info()"
      ],
      "metadata": {
        "id": "7mB8pALl_kLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive Statistics\n",
        "df.describe() generates descriptive statistics for your dataframe.\n",
        "- For numerical columns, it summarises the count, mean, measure of position (quartiles), and measures of variability (standard deviation, range from min/max values).\n",
        "\n",
        "- For text columns, it summarises the count, number of unique values (categories), the most frequent value and its frequency."
      ],
      "metadata": {
        "id": "jXOx0yTx51Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=\"all\") # all variables -- include=\"all\" is the default argument\n",
        "\n",
        "# df.describe(include=\"number\") # Only variables with integers/floats\n",
        "# df.describe(include=\"object\") # Only categorical variables with string data"
      ],
      "metadata": {
        "id": "5cEEWhVwJoJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Counts"
      ],
      "metadata": {
        "id": "3Cd1riNECvHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.describe() does not do a very good job at revealing useful information about your text columns.\n",
        "\n",
        ".value_counts() solves this by giving a count of each category within the specified column, making it easier to derive the same information as df.describe() but in greater detail."
      ],
      "metadata": {
        "id": "llUKifSN-DQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Education\"].value_counts())\n",
        "print() # leaves an empty line\n",
        "\n",
        "print(df[\"City\"].value_counts())\n",
        "print()\n",
        "\n",
        "print(df[\"Gender\"].value_counts())\n",
        "print()\n",
        "\n",
        "print(df[\"EverBenched\"].value_counts())\n",
        "print()\n",
        "\n",
        "# Response Variable\n",
        "print(df['LeaveOrNot'].value_counts())"
      ],
      "metadata": {
        "id": "YNHT-o1S8IK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unique Values"
      ],
      "metadata": {
        "id": "1gTkdeR5mCWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df.nunique()** provides the count of distinct values in each column.\n",
        "\n",
        "- Easier to spot mistakes\n",
        "  - e.g. We expect “Gender” to have 2 unique values but we see 4\n",
        "\n",
        "- Useful for discovering unexpected duplicates\n",
        "  - e.g. We expect 100k unique data points in your dataset but we only see 99k unique values"
      ],
      "metadata": {
        "id": "nJ9kAguEkn7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No. of unique elements in each column\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "O92eap_4ASCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".unique() can be used to check the unique elements in each column.\n",
        "- One advantage is that you can now see NaN values which .value_counts() could not show.\n",
        "\n",
        "To perform the same task as what we have done for \"Education\", we can use a \"for loop\" to execute the same block of code repeatedly."
      ],
      "metadata": {
        "id": "TAglKDgvDE6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique elements for a specific column\n",
        "print(df[\"Education\"].unique())\n",
        "print()\n",
        "\n",
        "# Check unique elements for all columns\n",
        "for col in df.columns:  # runs lines 7 - 9 for all column names\n",
        "  print(col)\n",
        "  print(df[col].unique())\n",
        "  print()"
      ],
      "metadata": {
        "id": "JHzIb1IqjUCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "\n",
        "From the inspection we have done, there are a few things that stand out:\n",
        "1. Age has NaN and negative values.\n",
        "2. Gender has two ways of classifying each gender.\n",
        "3. ExperienceInCurrentDomain has an unrealistic number (erroneous value).\n",
        "4. Education has NaN.\n",
        "\n",
        "We will take note of this and clean the dataset."
      ],
      "metadata": {
        "id": "-niV5b4x81eS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicated Values"
      ],
      "metadata": {
        "id": "HMrIjXrZl81p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to check for duplicates within our data as a first step. Repeated data are redundant and will hold more weight within the dataset so they can skew the analysis. This leads to problems like\n",
        "1. Inflated / deflated counts of certain values such as mean, std\n",
        "2. Incorrect visual representations of data\n",
        "    - Customer purchase recorded twice --> artificially higher transaction count --> flawed image of business performance\n",
        "3. Overfitting in machine learning\n",
        "    - Some data points appear more common than they actually are so the model's parameters are overly tuned to these particular points instead of learning the general pattern of the data"
      ],
      "metadata": {
        "id": "OD7-l0uJHRD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows the duplicated rows\n",
        "df[df.duplicated()]\n",
        "\n",
        "# The default argument is keep=\"first\" which only shows the second and subsequent duplicates\n",
        "# keep=False will include the first one to give the complete set of duplicates\n",
        "# df[df.duplicated(keep=False)]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sQIWhqY4CDLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above, we get a count of 1544 duplicated rows (excessive) so we use df.drop_duplicates() to remove them.\n",
        "\n",
        ".copy() tells Pandas explicitly to make a unique copy of the old dataframe i.e. changing values in the new dataframe does not change values in the old dataframe."
      ],
      "metadata": {
        "id": "-yEZxJ8bGx4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes duplicates and keeps unique rows\n",
        "df_clean = df.drop_duplicates().copy()  # creates a new dataframe without the duplicates\n",
        "df_clean.info()"
      ],
      "metadata": {
        "id": "nh50prx6iujd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original index labels still remain so RangeIndex has values from 0 to 4651. We want our row indexes to not have jumps in the sequence."
      ],
      "metadata": {
        "id": "RyvJlOvcHyBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset indexes\n",
        "df_clean.reset_index(drop=True, inplace=True) # drop=True ensures that the old index is not added as a column by itself\n",
        "df_clean.info()"
      ],
      "metadata": {
        "id": "pHaV2T68Hxcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there's still duplicated values\n",
        "print(df_clean.duplicated().sum())"
      ],
      "metadata": {
        "id": "w8FvKOD1IVXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inconsistent Labelling"
      ],
      "metadata": {
        "id": "JwVZFUP8KRdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean[\"Gender\"].value_counts())"
      ],
      "metadata": {
        "id": "QzZ3h8eBKUVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use .replace(old, new) to find all instances of \"old\" to replace with \"new\".\n",
        "\n",
        ".loc is a label-based indexer that allows you to select data based on the row and column labels / names.\n",
        "\n",
        "We can use : to select all the rows in the dataframe."
      ],
      "metadata": {
        "id": "BdXcSDmKLQZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the updated column (right) back into the original column (left)\n",
        "df_clean.loc[:, \"Gender\"] = df_clean[\"Gender\"].replace(\"m\", \"Male\")\n",
        "df_clean.loc[:, \"Gender\"] = df_clean[\"Gender\"].replace(\"F\", \"Female\")\n",
        "print(df_clean[\"Gender\"].value_counts())"
      ],
      "metadata": {
        "id": "VZXml3i_Kc-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Erroneous Values & Missing Values\n",
        "\n",
        "These 2 types of data quality issues involve changing values. There are 3 main methods to address these problems.\n",
        "1. Delete rows with missing values\n",
        "2. Impute using mean/median/mode (Univariate -- use only info from the same column to fill in missing values)\n",
        "3. Advanced imputation methods like KNN imputation (Multivariate -- use info from other columns to fill in missing values)\n",
        "\n",
        "Imputation --> assign values by inference"
      ],
      "metadata": {
        "id": "lSRgdavfN39G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Value Check"
      ],
      "metadata": {
        "id": "sRujTLvcl_5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to check if there are any missing values within each column.\n",
        "\n",
        "df.isna() checks if the column values are empty or not.\n",
        "- Empty --> True\n",
        "- Not empty --> False\n",
        "- Taking the sum for each column = Summing the True's = Counting the empty cells"
      ],
      "metadata": {
        "id": "-nAFh8YTCnRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts the missing values\n",
        "print(df_clean.isna().sum())   # df.isna().any() gives True / False to indicate if the column has empty values or not"
      ],
      "metadata": {
        "id": "P343EalrChmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing for Categorical Variables"
      ],
      "metadata": {
        "id": "KN4e7HNvmKIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean[\"Education\"].value_counts())\n",
        "print(f\"Number of NaN values:{(df_clean['Education']).isna().sum()}\" )  # f\"...\" is called f-string.\n",
        "print()\n",
        "\n",
        "# Also, note that .count() only considers non NaN values\n",
        "# 2213 + 704 + 161 = 3078 (does not include the 31 NaN values!)\n",
        "print(f\"Total Count of rows: {df_clean['Education'].count()}\")"
      ],
      "metadata": {
        "id": "WR6ZzQEymarj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have checked that there are 31 missing values, we can proceed to impute the missing values. There are two ways to do this:\n",
        "1. Impute the most frequent value\n",
        "2. Impute the value \"Missing\" such that Python treats it as a separate category\n",
        "\n",
        "Since there are few missing values relative to the size of the dataset, we will choose to impute the most frequent value."
      ],
      "metadata": {
        "id": "DqqVebZkR5nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer  # sklearn library supports imputation\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "df_clean[\"Education\"] = imputer.fit_transform(df_clean[[\"Education\"]]).ravel()  # Note the two square brackets\n",
        "\n",
        "# .ravel() flattens the 2D numPy array to 1D --> (no. of rows, )"
      ],
      "metadata": {
        "id": "tLbHvSt0TQ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All NaN values have been changed to \"Bachelors\" as shown in the code below."
      ],
      "metadata": {
        "id": "64VTLYtjUw69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean[\"Education\"].value_counts())\n",
        "print(f\"Number of NaN values: {df_clean['Education'].isna().sum()}\")"
      ],
      "metadata": {
        "id": "CdLcBtI5UgnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing for Quantitative Variables\n",
        "\n",
        "For quantitative variables, some data quality issues we may face are missing values, and negative values. In particular for missing values, there are numerous ways to deal with them for quantitative variables. What is important here is to choose a method based on how the data is missing (Missing Completely At Random, Missing At Random, Missing Not At Random), the percentage of missing values and the type of missing value.\n",
        "\n",
        "Some ways to deal with missing values are:\n",
        "1. Delete the rows with the missing values\n",
        "2. Use mean/mode/median imputation\n",
        "3. Use advanced imputation techniques like KNN imputation\n",
        "\n",
        "In general, it is a good idea to start with columns with few or too many missing values.\n",
        "\n"
      ],
      "metadata": {
        "id": "NjCePTdLd0W6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ExperienceInCurrentDomain"
      ],
      "metadata": {
        "id": "Y7hjJkhDpnRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean[\"ExperienceInCurrentDomain\"].value_counts())"
      ],
      "metadata": {
        "id": "yJERw1driOMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the investigation above, we can see that there are erroneous values in ExperienceInCurrentDomain.\n",
        "\n",
        "The most likely reason for this is a typo -- it could have been 20 or 2.\n",
        "Since 200 can skew the mean significantly, we will impute values based on the mode/median instead."
      ],
      "metadata": {
        "id": "xqqkcKrljlGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean[\"ExperienceInCurrentDomain\"] = df_clean[\"ExperienceInCurrentDomain\"].replace(200, np.nan)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "df_clean[\"ExperienceInCurrentDomain\"] = imputer.fit_transform(df_clean[[\"ExperienceInCurrentDomain\"]]).ravel()"
      ],
      "metadata": {
        "id": "B_hR-6mGk0Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean[\"ExperienceInCurrentDomain\"].value_counts())"
      ],
      "metadata": {
        "id": "MjZfTdldlvgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Age\n",
        "In the column Age, we have negative values and NaN."
      ],
      "metadata": {
        "id": "8ATbl9NrnFq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean[\"Age\"].value_counts())\n",
        "print()\n",
        "print(df_clean[\"Age\"].isna().sum())"
      ],
      "metadata": {
        "id": "Fk3IgWfzmZI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is likely that the negative values were simply recorded wrongly. The assumption here is that the errors are not systematic (for instance they are not occurring only for certain data collectors).\n",
        "\n",
        "For this, we have a larger number of missing data (169). While it's still somewhat small compared to the size of the dataset (6.56%), it is a lot larger than what we have been dealing with. To be cautious, we can consider more sophisticated imputation methods.\n",
        "\n",
        "kNN imputation finds the closest data points (neighbours) based on available features and uses their values to estimate the missing value."
      ],
      "metadata": {
        "id": "20hPIlCmpepl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Change negative values to NaN\n",
        "df_clean[\"Age\"] = df_clean[\"Age\"].replace([-9.0, -1.0], np.nan)\n",
        "\n",
        "\n",
        "# Step 2: OneHotEncoding\n",
        "categorical_col = ['Education', 'City', 'Gender', 'EverBenched']\n",
        "df_encoded = pd.get_dummies(df_clean, columns=categorical_col, drop_first=True)\n",
        "\n",
        "\n",
        "# Step 3: Normalisation\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "col_to_normalise = [\"Age\", \"ExperienceInCurrentDomain\", \"PaymentTier\", \"JoiningYear\"]\n",
        "df_encoded[col_to_normalise] = scaler.fit_transform(df_encoded[col_to_normalise])   # normalise quantitative variables\n",
        "\n",
        "\n",
        "# Step 4: KNN Imputation\n",
        "from sklearn.impute import KNNImputer\n",
        "knn_imputer = KNNImputer(n_neighbors=55)   # general rule of thumb = sqrt(N) where N is the no. of rows\n",
        "\n",
        "df_for_imputation = df_encoded.drop(columns=[\"LeaveOrNot\"])  # remove column\n",
        "array_knn = knn_imputer.fit_transform(df_for_imputation)\n",
        "df_clean_imputed = pd.DataFrame(array_knn, columns=df_for_imputation.columns)  # convert to DataFrame\n",
        "df_clean_imputed[\"LeaveOrNot\"] = df_encoded[\"LeaveOrNot\"]   # add back column\n",
        "\n",
        "\n",
        "# Step 5: Convert quantitative variables back to their original scale\n",
        "# The scaler stored the original min/max values for each column\n",
        "array_unscaled = scaler.inverse_transform(df_clean_imputed[col_to_normalise])   # reverses the scaling process\n",
        "\n",
        "# We convert the array to a DataFrame\n",
        "df_clean_unscaled = pd.DataFrame(array_unscaled, columns=col_to_normalise)\n",
        "\n",
        "\n",
        "# Step 6: Round values to integers\n",
        "df_clean_unscaled[\"Age\"] = df_clean_unscaled[\"Age\"].astype(int)\n",
        "\n",
        "\n",
        "# Step 7: Update the \"Age\" column in the original DataFrame\n",
        "df_clean[\"Age\"] = df_clean_unscaled[\"Age\"]"
      ],
      "metadata": {
        "id": "YuMPkGv3pfSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no more negative values!"
      ],
      "metadata": {
        "id": "D0sYE8Hn3PmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean[\"Age\"].isna().sum())\n",
        "print(df_clean[\"Age\"].value_counts())"
      ],
      "metadata": {
        "id": "ehRJhSNkp94F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Checks"
      ],
      "metadata": {
        "id": "QqAxJEwY6nyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.info()\n",
        "for col in df_clean.columns:\n",
        "  print(col)\n",
        "  print(df_clean[col].unique())\n",
        "  print()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z7Tr2uAG6Kmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now export the cleaned dataset."
      ],
      "metadata": {
        "id": "ojGBbpmO6IEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.to_csv(\"employee_cleaned.csv\", index=False)  # Warning: you should have run *ALL* code blocks before running this"
      ],
      "metadata": {
        "id": "A5FYM8hr5_84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YData Profile Report"
      ],
      "metadata": {
        "id": "TumKeTTnmbeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata_profiling\n",
        "from ydata_profiling import ProfileReport\n",
        "profile = ProfileReport(df, title=\"Employee\", explorative=True)\n",
        "profile.to_file(\"your_report.html\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('your_report.html')"
      ],
      "metadata": {
        "id": "_FwPzEeYELO5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For YData Report Verification\n",
        "print(f\"Duplicates from Python (including the first one): {df[df.duplicated(keep=False)].shape[0]}\")\n",
        "print(f\"Duplicates from Python: {df[df.duplicated()].shape[0]}\")\n",
        "print(f\"Duplicates from YData: {df[df.duplicated(keep=False)].shape[0] - df[df.duplicated()].shape[0]}\")"
      ],
      "metadata": {
        "id": "6z6eFEN3fmAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2193-1544 = 649. Verified that YData duplicate count is correct!"
      ],
      "metadata": {
        "id": "z59wRVtvb856"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miscellaneous\n",
        "Code for generating dirty dataset"
      ],
      "metadata": {
        "id": "eWPCn21zCd3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Change to NaN for Age (Missing Values)\n",
        "rows_to_change_nan = df.sample(n = 200, random_state=2040).index\n",
        "df.loc[rows_to_change_nan, \"Age\"] = np.nan\n",
        "\n",
        "# Change 2 in ExperienceInCurrentDomain to 200 (Erroneous Values)\n",
        "df_filtered = df[df[\"ExperienceInCurrentDomain\"] == 2]  # df with rows that have exp = 2 years\n",
        "rows_to_change_2 = df_filtered.sample(n = 54, random_state=2040).index\n",
        "df.loc[rows_to_change_2, \"ExperienceInCurrentDomain\"] = 200\n",
        "\n",
        "# Change Male in Gender to m and Female in Gender to F (Inconsistent Labelling)\n",
        "df_male = df[df[\"Gender\"] == \"Male\"]\n",
        "rows_to_change_male = df_male.sample(n = 378, random_state=2040).index\n",
        "df.loc[rows_to_change_male, \"Gender\"] = \"m\"\n",
        "\n",
        "df_female = df[df[\"Gender\"] == \"Female\"]\n",
        "rows_to_change_female = df_female.sample(n = 103, random_state=2040).index\n",
        "df.loc[rows_to_change_female, \"Gender\"] = \"F\"\n",
        "\n",
        "# Change 26 to -1 and -9 for Age (Negative Values)\n",
        "df_young = df[df[\"Age\"] == 26]\n",
        "rows_to_change_young = df_young.sample(n = 43, random_state=2040).index\n",
        "df.loc[rows_to_change_young, \"Age\"] = -1\n",
        "\n",
        "rows_to_change_young2 = df_young.sample(n = 39, random_state=2040).index\n",
        "df.loc[rows_to_change_young2, \"Age\"] = -9\n",
        "\n",
        "# Change Bachelors to NaN for Education (Categorical Variables)\n",
        "df_bachelors = df[df[\"Education\"] == \"Bachelors\"]\n",
        "rows_to_change_bachelors = df_bachelors.sample(n = 32, random_state=2040).index\n",
        "df.loc[rows_to_change_bachelors, \"Education\"] = np.nan\n",
        "'''"
      ],
      "metadata": {
        "id": "MZZbdBKsCdRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv(\"employee_dirty.csv\", index=False, na_rep=\"\")"
      ],
      "metadata": {
        "id": "tD8xy2maCldk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}